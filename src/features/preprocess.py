# src/features/preprocess.py
import pandas as pd
import numpy as np
import os
import joblib  # Scaler'Ä± kaydetmek iÃ§in gerekli
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

def process_full_pipeline():
    # 1. DOSYA YOLLARI
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(current_dir))
    base_path = os.path.join(project_root, "data", "processed_csv")
    
    file_list = [
        "Monday-WorkingHours.pcap_ISCX.csv",
        "Tuesday-WorkingHours.pcap_ISCX.csv",
        "Wednesday-workingHours.pcap_ISCX.csv",
        "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv",
        "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv",
        "Friday-WorkingHours-Morning.pcap_ISCX.csv",
        "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv",
        "Friday-WorkingHours-Afternoon-DDoS.pcap_ISCX.csv"
    ]

    print(f"ğŸš€ DERÄ°N ALTYAPI MODU: Toplam {len(file_list)} adet dosya iÅŸlenecek...")

    # 2. YÃœKLEME VE BÄ°RLEÅTÄ°RME
    dfs = []
    for f in file_list:
        path = os.path.join(base_path, f)
        if os.path.exists(path):
            print(f"   Reading: {f} ...")
            try:
                df = pd.read_csv(path, encoding='latin1', low_memory=False)
                df.columns = df.columns.str.strip() # BoÅŸluklarÄ± temizle
                dfs.append(df)
            except Exception as e:
                print(f"   HATA: {f} okunamadÄ±. Sebebi: {e}")
        else:
            print(f"   UYARI: {path} bulunamadÄ±!")

    if not dfs:
        print("âŒ HiÃ§ veri yÃ¼klenemedi. Ä°ÅŸlem iptal.")
        return

    full_data = pd.concat(dfs, ignore_index=True)
    print(f"ğŸ“Š BÄ°RLEÅTÄ°RÄ°LMÄ°Å HAM VERÄ°: {full_data.shape}")

    # 3. KÄ°MLÄ°K SÃœTUNLARINI ATMA (Overfitting Ã–nlemi)
    # Modelin 'DavranÄ±ÅŸÄ±' Ã¶ÄŸrenmesi iÃ§in 'Kimlikleri' siliyoruz.
    drop_cols = [
        'Flow ID', 
        'Source IP', 'Src IP', 
        'Source Port', 'Src Port', 
        'Destination IP', 'Dest IP', 
        'Destination Port', 'Dest Port', 
        'Timestamp', 'Date'
    ]
    
    # Sadece veride mevcut olan sÃ¼tunlarÄ± sil
    existing_drop_cols = [c for c in drop_cols if c in full_data.columns]
    print(f"ğŸ—‘ï¸ Gereksiz sÃ¼tunlar siliniyor: {len(existing_drop_cols)} adet")
    full_data.drop(columns=existing_drop_cols, inplace=True)

    # 4. TEMÄ°ZLÄ°K
    print("ğŸ§¹ Temizlik yapÄ±lÄ±yor (NaN ve Sonsuz deÄŸerler)...")
    full_data.replace([np.inf, -np.inf], np.nan, inplace=True)
    full_data.dropna(inplace=True)

    print("ğŸ”„ Tekrarlayan veriler temizleniyor...")
    full_data.drop_duplicates(inplace=True)
    print(f"   Temizlik sonrasÄ±: {full_data.shape}")

    # 5. ETÄ°KETLEME
    print("ğŸ·ï¸ Etiketler iÅŸleniyor...")
    y = full_data['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)
    X = full_data.drop(['Label'], axis=1)

    # 6. BÃ–LME (Splitting) - Ã–NCE BÃ–L, SONRA SCALE ET!
    print("âœ‚ï¸ Veri setleri bÃ¶lÃ¼nÃ¼yor (%70 - %15 - %15)...")
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.30, random_state=42, stratify=y
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp
    )

    # 7. Ã–LÃ‡EKLEME (Scaling) - KRÄ°TÄ°K ADIM
    # MinMaxScaler verileri 0-1 arasÄ±na sÄ±kÄ±ÅŸtÄ±rÄ±r. Deep Learning iÃ§in en iyisidir.
    print("âš–ï¸ Veriler Ã¶lÃ§ekleniyor (MinMax Scaling)...")
    
    scaler = MinMaxScaler()
    
    # Scaler SADECE eÄŸitim verisini gÃ¶rmeli (Fit)
    # Sonra diÄŸerlerini dÃ¶nÃ¼ÅŸtÃ¼rmeli (Transform)
    # Bunu yapmazsak 'Data Leakage' olur.
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    # Scaler'Ä± kaydet (CanlÄ± sistemde kullanmak iÃ§in ÅŸart!)
    scaler_path = os.path.join(project_root, "models", "scaler.pkl")
    if not os.path.exists(os.path.dirname(scaler_path)):
        os.makedirs(os.path.dirname(scaler_path))
    joblib.dump(scaler, scaler_path)
    print(f"ğŸ’¾ Scaler kaydedildi: {scaler_path}")

    # DataFrame'e geri Ã§evir (SÃ¼tun isimlerini korumak iÃ§in)
    columns = X.columns
    X_train = pd.DataFrame(X_train_scaled, columns=columns)
    X_val = pd.DataFrame(X_val_scaled, columns=columns)
    X_test = pd.DataFrame(X_test_scaled, columns=columns)

    # 8. KAYDETME
    save_dir = os.path.join(base_path, "ready_splits")
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    print("ğŸ’¾ Ä°ÅŸlenmiÅŸ veriler diske yazÄ±lÄ±yor...")
    
    # Index resetlemek Ã¶nemli, yoksa concat hata verir
    y_train = y_train.reset_index(drop=True)
    y_val = y_val.reset_index(drop=True)
    y_test = y_test.reset_index(drop=True)

    pd.concat([X_train, y_train], axis=1).to_csv(os.path.join(save_dir, "train.csv"), index=False)
    pd.concat([X_val, y_val], axis=1).to_csv(os.path.join(save_dir, "val.csv"), index=False)
    pd.concat([X_test, y_test], axis=1).to_csv(os.path.join(save_dir, "test.csv"), index=False)

    print(f"ğŸ Ä°ÅLEM TAMAM! Dosyalar ÅŸurada hazÄ±r: {save_dir}")

if __name__ == "__main__":
    process_full_pipeline()
