{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "RANDOM_STATE = 42\n",
        "SAMPLE_SIZE = 200_000\n",
        "TARGET_COLUMN = 'Label'\n",
        "\n",
        "# Paths\n",
        "PROJECT_ROOT = Path('..').resolve()\n",
        "DATA_PATH = PROJECT_ROOT / 'data' / 'processed_randomforest' / 'train.csv'\n",
        "RESULTS_DIR = PROJECT_ROOT / 'experiments' / 'results'\n",
        "MODELS_DIR = PROJECT_ROOT / 'models'\n",
        "\n",
        "# Ensure output directories exist\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"ğŸ“‚ Data Path: {DATA_PATH}\")\n",
        "print(f\"ğŸ“Š Results Dir: {RESULTS_DIR}\")\n",
        "print(f\"ğŸ¤– Models Dir: {MODELS_DIR}\")\n",
        "print(f\"âœ… Configuration loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the pre-processed dataset\n",
        "print(\"â³ Loading dataset (this may take a moment for large files)...\")\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(f\"âœ… Dataset loaded! Shape: {df.shape}\")\n",
        "print(f\"   Total samples: {df.shape[0]:,}\")\n",
        "print(f\"   Features: {df.shape[1] - 1}\")\n",
        "print(f\"   Target: '{TARGET_COLUMN}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display original class distribution\n",
        "print(\"\\nğŸ“Š Original Class Distribution:\")\n",
        "print(\"=\" * 50)\n",
        "class_dist = df[TARGET_COLUMN].value_counts().sort_index()\n",
        "class_pct = df[TARGET_COLUMN].value_counts(normalize=True).sort_index() * 100\n",
        "\n",
        "for label, count in class_dist.items():\n",
        "    pct = class_pct[label]\n",
        "    print(f\"  Class {label}: {count:>10,} samples ({pct:>6.2f}%)\")\n",
        "\n",
        "print(f\"\\n  Total: {df.shape[0]:,} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategic Sampling with Stratification\n",
        "print(f\"\\nâš¡ Creating stratified sample of {SAMPLE_SIZE:,} rows...\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[TARGET_COLUMN])\n",
        "y = df[TARGET_COLUMN]\n",
        "\n",
        "# Calculate sample fraction\n",
        "sample_frac = SAMPLE_SIZE / len(df)\n",
        "\n",
        "# Stratified sampling using train_test_split\n",
        "# We take SAMPLE_SIZE as our \"train\" portion\n",
        "X_sample, _, y_sample, _ = train_test_split(\n",
        "    X, y,\n",
        "    train_size=SAMPLE_SIZE,\n",
        "    stratify=y,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# Combine back into single DataFrame for PyCaret\n",
        "df_sample = pd.concat([X_sample, y_sample], axis=1).reset_index(drop=True)\n",
        "\n",
        "print(f\"âœ… Stratified sample created! Shape: {df_sample.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify stratification integrity\n",
        "print(\"\\nğŸ” Sample Class Distribution (Integrity Check):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "sample_dist = df_sample[TARGET_COLUMN].value_counts().sort_index()\n",
        "sample_pct = df_sample[TARGET_COLUMN].value_counts(normalize=True).sort_index() * 100\n",
        "original_pct = class_pct\n",
        "\n",
        "print(f\"{'Class':<10} {'Count':>12} {'Sample %':>12} {'Original %':>12} {'Î”':>8}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for label in sample_dist.index:\n",
        "    count = sample_dist[label]\n",
        "    s_pct = sample_pct[label]\n",
        "    o_pct = original_pct[label]\n",
        "    delta = s_pct - o_pct\n",
        "    print(f\"{label:<10} {count:>12,} {s_pct:>11.2f}% {o_pct:>11.2f}% {delta:>+7.3f}%\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Total':<10} {df_sample.shape[0]:>12,}\")\n",
        "print(\"\\nâœ… Stratification preserved! Class proportions maintained.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Free up memory from original dataset\n",
        "del df, X, y\n",
        "import gc\n",
        "gc.collect()\n",
        "print(\"ğŸ§¹ Memory cleaned: Original dataset removed from memory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pycaret.classification import (\n",
        "    setup, compare_models, plot_model, save_model, create_model, tune_model\n",
        ")\n",
        "\n",
        "print(\"ğŸš€ Initializing PyCaret Classification Environment...\")\n",
        "print(\"   GPU Mode: DISABLED (Intel OpenCL Issue)\")\n",
        "print(\"   Normalization: ENABLED\")\n",
        "print(\"   Session ID: 42 (Reproducibility)\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize PyCaret setup\n",
        "clf_setup = setup(\n",
        "    data=df_sample,\n",
        "    target=TARGET_COLUMN,\n",
        "    session_id=RANDOM_STATE,\n",
        "    n_jobs=-1,              # Use all CPU cores\n",
        "    use_gpu=False,          # GPU disabled to prevent LightGBM/Intel OpenCL crash\n",
        "    normalize=True,         # Apply scaling\n",
        "    log_experiment=False,   # Lightweight - no MLflow logging\n",
        "    verbose=True,           # Show setup information\n",
        "    html=False              # Disable HTML output for cleaner logs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define models to compare\n",
        "BATTLE_MODELS = [\n",
        "    'lightgbm',   # LightGBM - Fast gradient boosting\n",
        "    'xgboost',    # XGBoost - Extreme gradient boosting\n",
        "    'catboost',   # CatBoost - Categorical boosting\n",
        "    'rf',         # Random Forest\n",
        "    'et',         # Extra Trees\n",
        "    'dt',         # Decision Tree\n",
        "    'ada',        # AdaBoost\n",
        "    'gbc'         # Gradient Boosting Classifier\n",
        "]\n",
        "\n",
        "print(\"âš”ï¸  THE BATTLE BEGINS!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Contenders:\")\n",
        "for i, model in enumerate(BATTLE_MODELS, 1):\n",
        "    print(f\"  {i}. {model.upper()}\")\n",
        "print(\"\\nğŸ“ Metrics: 5-Fold CV | Optimizing for F1 Score\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the comparison\n",
        "print(\"\\nğŸ Starting model comparison (this may take several minutes)...\\n\")\n",
        "\n",
        "best_model = compare_models(\n",
        "    include=BATTLE_MODELS,\n",
        "    fold=5,                 # 5-fold CV for speed\n",
        "    sort='F1',              # Optimize for F1 Score\n",
        "    n_select=1,             # Return only the best model\n",
        "    verbose=True            # Show progress\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ† CHAMPION SELECTED!\")\n",
        "print(f\"   Best Model: {type(best_model).__name__}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "\n",
        "print(\"ğŸ“Š Generating visualizations...\")\n",
        "print(f\"   Output directory: {RESULTS_DIR}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to save PyCaret plots\n",
        "def save_pycaret_plot(model, plot_type, filename):\n",
        "    \"\"\"\n",
        "    Generate and save PyCaret plot.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained PyCaret model\n",
        "        plot_type: Type of plot (e.g., 'confusion_matrix', 'auc', 'feature')\n",
        "        filename: Output filename (without extension)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # PyCaret's plot_model saves to current directory by default\n",
        "        plot_model(model, plot=plot_type, save=True)\n",
        "        \n",
        "        # Default name pattern used by PyCaret\n",
        "        default_name = f\"{plot_type.replace('_', ' ').title()}.png\"\n",
        "        \n",
        "        # Alternative patterns PyCaret might use\n",
        "        possible_names = [\n",
        "            default_name,\n",
        "            f\"{plot_type}.png\",\n",
        "            f\"{plot_type.title()}.png\",\n",
        "            \"Confusion Matrix.png\",\n",
        "            \"Feature Importance.png\",\n",
        "            \"AUC.png\"\n",
        "        ]\n",
        "        \n",
        "        # Find and move the generated file\n",
        "        for name in possible_names:\n",
        "            if os.path.exists(name):\n",
        "                target_path = RESULTS_DIR / filename\n",
        "                shutil.move(name, target_path)\n",
        "                print(f\"  âœ… Saved: {filename}\")\n",
        "                return target_path\n",
        "        \n",
        "        print(f\"  âš ï¸ Could not locate generated plot for {plot_type}\")\n",
        "        return None\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  âŒ Error generating {plot_type}: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate and save Confusion Matrix\n",
        "print(\"\\n Generating Confusion Matrix...\")\n",
        "save_pycaret_plot(best_model, 'confusion_matrix', 'confusion_matrix.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate and save Feature Importance\n",
        "print(\"\\n Generating Feature Importance...\")\n",
        "save_pycaret_plot(best_model, 'feature', 'feature_importance.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate and save AUC/ROC Curve\n",
        "print(\"\\n Generating ROC/AUC Curve...\")\n",
        "save_pycaret_plot(best_model, 'auc', 'auc.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display generated visualizations\n",
        "print(\"\\n Generated Artifacts:\")\n",
        "print(\"=\" * 60)\n",
        "for f in RESULTS_DIR.glob('*.png'):\n",
        "    print(f\"  ğŸ“Š {f.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the champion model\n",
        "model_path = MODELS_DIR / 'pycaret_champion'\n",
        "\n",
        "print(\"\\n Saving champion model...\")\n",
        "save_model(best_model, str(model_path))\n",
        "print(f\"\\n Model saved to: {model_path}\")\n",
        "print(f\"   File: pycaret_champion.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘           BENCHMARK COMPLETE - SUMMARY                     â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘                                                              â•‘\n",
        "â•‘   Data Pipeline:                                           â•‘\n",
        "â•‘     â”œâ”€ Source: data/processed_randomforest/train.csv         â•‘\n",
        "â•‘     â”œâ”€ Original: ~1.3M+ samples                              â•‘\n",
        "â•‘     â””â”€ Stratified Sample: 200,000 samples                    â•‘\n",
        "â•‘                                                              â•‘\n",
        "â•‘    Battle Results:                                          â•‘\n",
        "â•‘     â”œâ”€ Algorithms Tested: 8                                  â•‘\n",
        "â•‘     â”œâ”€ Cross-Validation: 5-Fold                              â•‘\n",
        "â•‘     â”œâ”€ Primary Metric: F1 Score                              â•‘\n",
        "â•‘     â””â”€ Champion: {champion_name:<35} â•‘\n",
        "â•‘                                                              â•‘\n",
        "â•‘   Artifacts Generated:                                     â•‘\n",
        "â•‘     â”œâ”€ experiments/results/confusion_matrix.png              â•‘\n",
        "â•‘     â”œâ”€ experiments/results/feature_importance.png            â•‘\n",
        "â•‘     â”œâ”€ experiments/results/auc.png                           â•‘\n",
        "â•‘     â””â”€ models/pycaret_champion.pkl                           â•‘\n",
        "â•‘                                                              â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘   Next Steps:                                               â•‘\n",
        "â•‘     1. Review metrics from compare_models() output           â•‘\n",
        "â•‘     2. Fine-tune hyperparameters with tune_model()           â•‘\n",
        "â•‘     3. Train on full dataset with identified architecture    â•‘\n",
        "â•‘     4. Evaluate on held-out test set                         â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\".format(champion_name=type(best_model).__name__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display model info\n",
        "print(\"\\n Champion Model Details:\")\n",
        "print(\"=\" * 60)\n",
        "print(best_model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.25"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
